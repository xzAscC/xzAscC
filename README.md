# Hi, I'm Xudong Zhu ðŸ‘‹

![Arch Linux](https://img.shields.io/badge/Arch%20Linux-1793D1?style=flat-square&logo=arch-linux&logoColor=white)
![Neovim](https://img.shields.io/badge/Neovim-57A143?style=flat-square&logo=neovim&logoColor=white)
![fish shell](https://img.shields.io/badge/fish%20shell-34C534?style=flat-square&logo=fish-shell&logoColor=white)
![OpenCode](https://img.shields.io/badge/OpenCode-AI%20Coding%20Tool-black?style=flat-square)
![Research](https://img.shields.io/badge/focus-LLM%20Interpretability-black?style=flat-square)

PhD student at The Ohio State University working on understanding and controlling large language models. I also vibe code research prototypes, developer tools, and AI systems.

## About Me

- I am a PhD student in Computer Science & Engineering at The Ohio State University.

- My research focuses on mechanistic interpretability, and sparse representations in LLMs.

- I study how internal representations encode concepts and how we can steer model behavior with interpretable directions.

- My goal is to make LLMs more transparent, controllable, and reliable.

## GitHub Stats

<p align="center">
  <img height="160" src="https://github-readme-stats-sigma-five.vercel.app/api?username=xzAscC&show_icons=true&rank_icon=github" />
  <img height="160" src="https://github-readme-stats-sigma-five.vercel.app/api/top-langs/?username=xzAscC&layout=compact" />
</p>

## Recent Projects

- Understanding Linear Steering (ongoing)  
  Investigating the geometry, linearity, and causal structure of steering directions in LLM representation space.

- AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features [ArXiv](https://arxiv.org/abs/2510.00404), [OpenReview](https://openreview.net/forum?id=EEs6I4cO7S)  
  Developed a principled proximal-gradient framework that unifies SAE variants (ReLU, JumpReLU, TopK) and reveals that non-negativity constraints prevent bidirectional feature representation. Proposed AbsTopK, a magnitude-based sparse operator that recovers complete semantic axes and improves interpretability and steering in LLMs.

- From Emergence to Control: Probing and Modulating Self-Reflection in Language Models  [Arxiv](https://arxiv.org/abs/2506.12217)  
  Showed that linear directions in representation space can enable and control self-reflection behavior in pretrained LLMs without finetuning.

For a full list of publications:

- [Google Scholar](https://scholar.google.com/citations?user=U55yracAAAAJ&hl=zh-CN)  
- [OpenReview](https://openreview.net/profile?id=%7EXudong_Zhu1)

---

If you are interested in collaboration, feel free to open an issue or connect with me.
